{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (12): ReLU(inplace=True)\n    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (14): ReLU(inplace=True)\n    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (17): ReLU(inplace=True)\n    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (19): ReLU(inplace=True)\n    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "import torch_pruning as tp\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "# model = torch.load('resnet/resnet18.pth')\n",
    "model = torch.load('../../vgg/vgg11.pth')\n",
    "# build layer dependency for resnet18\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random_int_list(start, stop, length):\n",
    "    start, stop = (int(start), int(stop)) if start <= stop else (int(stop), int(start))\n",
    "    length = int(abs(length)) if length else 0\n",
    "    random_list = []\n",
    "    for i in range(length):\n",
    "        random_list.append(random.randint(start, stop))\n",
    "    return random_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model):\n",
    "    # model.cpu()\n",
    "    # DG = tp.DependencyGraph(model, fake_input=torch.randn(1,3,224,224)).build_dependency( model, torch.randn(1, 3, 224, 224) )\n",
    "    DG = tp.DependencyGraph( model.cpu(), fake_input=torch.randn(1,3,224,224) )\n",
    "    def prune_conv(conv, pruned_prob):\n",
    "        weight = conv.weight.detach().cpu().numpy()\n",
    "        out_channels = weight.shape[0]\n",
    "        L1_norm = np.sum( np.abs(weight), axis=(1,2,3))\n",
    "        num_pruned = int(out_channels * pruned_prob)\n",
    "        prune_index = np.argsort(L1_norm)[:num_pruned].tolist() # remove filters with small L1-Norm\n",
    "        print(prune_index)\n",
    "        plan = DG.get_pruning_plan(conv, tp.prune_conv, prune_index)\n",
    "        plan.exec()\n",
    "    \n",
    "    conv_prune_probs = [0.1, 0.1, 0.2, 0.2, 0.2, 0.2, 0.3, 0.3]\n",
    "    blk_id = 0\n",
    "    for m in model.features:\n",
    "        if isinstance( m, nn.Conv2d ):\n",
    "            print(m)\n",
    "            prune_conv( m, conv_prune_probs[blk_id] )\n",
    "            blk_id+=1\n",
    "\n",
    "    #pruning Fully connective layer\n",
    "    # linear = model.classifier[0]\n",
    "    # weight = linear.weight.detach().cpu().numpy()\n",
    "    # out_channels = weight.shape[0]\n",
    "    # L1_norm = np.sum( np.abs(weight), axis=(1))\n",
    "    # num_pruned = int(out_channels * 0.2)\n",
    "    # prune_index = np.argsort(L1_norm)[:num_pruned].tolist() # remove filters with small L1-Norm\n",
    "\n",
    "    # pruning_plan = DG.get_pruning_plan( linear, tp.prune_linear, prune_index)\n",
    "    # pruning_plan.exec()\n",
    "\n",
    "    return model    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n[22, 30, 52, 31, 38, 15]\nConv2d(58, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n[8, 0, 42, 110, 22, 47, 5, 107, 88, 41, 37, 124]\nConv2d(116, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n[108, 48, 136, 201, 31, 86, 76, 179, 87, 175, 73, 251, 52, 137, 186, 191, 17, 176, 77, 7, 212, 13, 6, 229, 29, 217, 246, 50, 93, 218, 67, 90, 252, 63, 133, 49, 167, 100, 46, 157, 54, 66, 190, 222, 142, 232, 249, 245, 239, 145, 171]\nConv2d(205, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n[76, 206, 133, 231, 207, 175, 145, 31, 186, 115, 136, 179, 47, 104, 38, 184, 2, 122, 239, 34, 140, 217, 40, 50, 66, 81, 216, 188, 144, 79, 53, 196, 208, 215, 102, 48, 134, 236, 187, 151, 132, 101, 194, 64, 108, 181, 46, 116, 71, 109, 118]\nConv2d(205, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n[73, 355, 381, 436, 481, 321, 12, 254, 309, 243, 407, 131, 319, 212, 428, 78, 343, 287, 177, 367, 405, 292, 147, 372, 387, 87, 104, 411, 276, 501, 184, 270, 10, 174, 126, 227, 507, 33, 138, 465, 130, 510, 62, 419, 427, 173, 457, 107, 4, 158, 398, 11, 454, 190, 2, 19, 109, 64, 445, 181, 183, 83, 446, 493, 129, 14, 262, 117, 81, 24, 490, 3, 169, 108, 303, 22, 67, 136, 88, 443, 485, 433, 97, 89, 74, 293, 171, 133, 464, 455, 295, 168, 18, 509, 430, 231, 412, 273, 330, 25, 9, 402]\nConv2d(410, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n[397, 340, 486, 279, 142, 487, 268, 324, 418, 452, 450, 510, 152, 458, 212, 389, 192, 106, 117, 396, 132, 225, 94, 194, 133, 370, 451, 406, 348, 78, 284, 427, 400, 22, 41, 240, 131, 310, 420, 488, 432, 314, 218, 459, 271, 503, 103, 289, 326, 426, 148, 138, 147, 464, 175, 468, 493, 407, 134, 366, 352, 466, 274, 165, 181, 463, 319, 60, 355, 25, 435, 204, 369, 67, 79, 395, 196, 51, 93, 317, 276, 255, 424, 23, 475, 7, 281, 256, 351, 9, 105, 265, 1, 401, 249, 219, 433, 430, 290, 294, 179, 33]\nConv2d(410, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n[2, 210, 182, 355, 221, 98, 55, 162, 349, 140, 215, 87, 309, 40, 199, 70, 456, 381, 499, 38, 426, 403, 418, 361, 301, 334, 79, 507, 302, 490, 168, 336, 103, 54, 479, 82, 310, 354, 226, 116, 396, 223, 227, 96, 110, 346, 305, 327, 457, 25, 473, 278, 359, 173, 369, 16, 272, 100, 90, 152, 19, 119, 405, 377, 476, 156, 402, 124, 202, 146, 306, 122, 324, 316, 250, 255, 43, 498, 279, 61, 184, 277, 454, 347, 220, 400, 75, 165, 60, 335, 465, 104, 483, 74, 358, 432, 78, 266, 404, 433, 392, 273, 245, 319, 423, 496, 322, 345, 441, 422, 68, 401, 148, 53, 149, 31, 458, 440, 296, 362, 153, 212, 283, 34, 67, 395, 106, 229, 285, 350, 6, 318, 235, 506, 398, 351, 437, 297, 35, 414, 18, 477, 264, 137, 448, 455, 101, 312, 32, 88, 321, 109, 138]\nConv2d(359, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n[117, 54, 471, 428, 217, 402, 218, 384, 3, 105, 36, 481, 247, 430, 2, 511, 280, 309, 21, 427, 502, 496, 48, 267, 339, 322, 134, 444, 7, 482, 294, 446, 391, 405, 493, 51, 275, 79, 253, 180, 98, 13, 462, 26, 396, 494, 111, 135, 503, 198, 366, 213, 181, 470, 22, 371, 231, 375, 55, 92, 401, 110, 200, 505, 312, 190, 66, 475, 228, 120, 147, 214, 246, 301, 316, 336, 211, 337, 102, 500, 194, 359, 272, 250, 395, 460, 140, 195, 114, 118, 439, 144, 220, 90, 452, 299, 436, 454, 227, 459, 103, 248, 256, 435, 80, 19, 383, 29, 188, 372, 229, 390, 259, 183, 99, 477, 197, 456, 265, 440, 232, 310, 438, 57, 274, 237, 112, 413, 156, 173, 403, 441, 56, 410, 126, 434, 128, 457, 445, 422, 399, 169, 242, 397, 355, 370, 201, 249, 338, 293, 224, 42, 204]\n"
    }
   ],
   "source": [
    "model = prune_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(58, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(116, 205, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(205, 205, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (11): Conv2d(205, 410, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (12): ReLU(inplace=True)\n    (13): Conv2d(410, 410, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (14): ReLU(inplace=True)\n    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (16): Conv2d(410, 359, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (17): ReLU(inplace=True)\n    (18): Conv2d(359, 359, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (19): ReLU(inplace=True)\n    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=17591, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'../../vgg/pruning/vgg11.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}